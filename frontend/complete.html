<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ani - Complete Voice Companion</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Arial, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            overflow: hidden;
            color: white;
        }
        #canvas-container { width: 100vw; height: 100vh; position: relative; }
        #controls {
            position: absolute;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0, 0, 0, 0.7);
            padding: 20px 40px;
            border-radius: 50px;
            display: flex;
            gap: 15px;
            align-items: center;
            backdrop-filter: blur(10px);
        }
        button {
            padding: 12px 24px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border: none;
            border-radius: 25px;
            color: white;
            font-size: 14px;
            cursor: pointer;
            transition: all 0.3s;
            font-weight: 600;
        }
        button:hover { transform: translateY(-2px); }
        button.recording {
            background: linear-gradient(135deg, #f44336, #e91e63);
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }
        #status {
            position: absolute;
            top: 20px;
            left: 20px;
            background: rgba(0, 0, 0, 0.7);
            padding: 15px 25px;
            border-radius: 15px;
            font-size: 14px;
            backdrop-filter: blur(10px);
        }
        .status-dot {
            display: inline-block;
            width: 10px;
            height: 10px;
            border-radius: 50%;
            margin-right: 8px;
        }
        .status-connected { background: #4CAF50; }
        .status-disconnected { background: #f44336; }
        .status-thinking { background: #FFC107; }
        #emotion-display {
            position: absolute;
            top: 20px;
            right: 20px;
            background: rgba(0, 0, 0, 0.7);
            padding: 15px 25px;
            border-radius: 15px;
            backdrop-filter: blur(10px);
        }
        #loading {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            text-align: center;
            font-size: 24px;
            font-weight: 600;
        }
        .spinner {
            border: 4px solid rgba(255, 255, 255, 0.3);
            border-top: 4px solid white;
            border-radius: 50%;
            width: 50px;
            height: 50px;
            animation: spin 1s linear infinite;
            margin: 20px auto;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body>
    <div id="canvas-container"></div>
    <div id="loading"><div class="spinner"></div>Loading Ani...</div>
    <div id="status" style="display: none;">
        <span class="status-dot status-disconnected"></span>
        <span id="status-text">Disconnected</span>
    </div>
    <div id="emotion-display" style="display: none;">
        <div style="opacity: 0.7; font-size: 12px; margin-bottom: 5px;">Emotion</div>
        <div id="emotion-text">Neutral</div>
    </div>
    <div id="controls" style="display: none;">
        <button id="talk-btn">Hold to Talk</button>
        <span id="mic-status" style="font-size: 12px; opacity: 0.7;">Click and speak</span>
    </div>

    <script type="importmap">
    {
        "imports": {
            "three": "https://cdn.jsdelivr.net/npm/three@0.167.0/build/three.module.js",
            "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.167.0/examples/jsm/",
            "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3.1.2/lib/three-vrm.module.js"
        }
    }
    </script>

    <script type="module">
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { VRMLoaderPlugin } from '@pixiv/three-vrm';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';

        // Global state
        let scene, camera, renderer, controls, vrm;
        let ws = null;
        let recognition = null;
        let isListening = false;
        let currentAudio = null;
        const clock = new THREE.Clock();
        let time = 0;

        const EMOTION_MAP = {
            'joy': 'happy',
            'sad': 'sad',
            'anger': 'angry',
            'surprise': 'surprised',
            'neutral': 'neutral'
        };

        // 初始化场景
        scene = new THREE.Scene();
        scene.background = new THREE.Color(0x212121);

        camera = new THREE.PerspectiveCamera(30, window.innerWidth / window.innerHeight, 0.1, 100);
        camera.position.set(0, 1.4, 3);

        renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.setPixelRatio(window.devicePixelRatio);
        renderer.outputColorSpace = THREE.SRGBColorSpace;
        document.getElementById('canvas-container').appendChild(renderer.domElement);

        // 灯光
        const light1 = new THREE.DirectionalLight(0xffffff, 1.5);
        light1.position.set(1, 1, 1);
        scene.add(light1);

        const light2 = new THREE.AmbientLight(0xffffff, 0.8);
        scene.add(light2);

        const rimLight = new THREE.DirectionalLight(0x667eea, 0.5);
        rimLight.position.set(-1, 1, -1);
        scene.add(rimLight);

        // 控制器
        controls = new OrbitControls(camera, renderer.domElement);
        controls.target.set(0, 1.2, 0);
        controls.enableDamping = true;
        controls.dampingFactor = 0.05;
        controls.update();

        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        // 加载VRM
        const loader = new GLTFLoader();
        loader.register((parser) => new VRMLoaderPlugin(parser));

        loader.load('/character/darkhair.vrm', (gltf) => {
            vrm = gltf.userData.vrm;
            scene.add(vrm.scene);
            vrm.scene.rotation.y = Math.PI;

            console.log('[OK] VRM loaded');

            if (vrm.humanoid) {
                // 设置初始姿势 - 使用 rotation.set() 而不是分别设置 x/y/z
                const leftArm = vrm.humanoid.getNormalizedBoneNode('leftUpperArm');
                const rightArm = vrm.humanoid.getNormalizedBoneNode('rightUpperArm');

                if (leftArm) {
                    leftArm.rotation.set(0, 0, 1.5);  // 手臂向下
                    console.log('[OK] Left arm posed');
                }
                if (rightArm) {
                    rightArm.rotation.set(0, 0, -1.5);  // 手臂向下
                    console.log('[OK] Right arm posed');
                }
            }

            document.getElementById('loading').style.display = 'none';
            document.getElementById('controls').style.display = 'flex';
            document.getElementById('status').style.display = 'block';
            document.getElementById('emotion-display').style.display = 'block';
        });

        // 表情控制
        function setExpression(emotion, intensity = 1.0) {
            if (!vrm || !vrm.expressionManager) return;

            const expressionName = EMOTION_MAP[emotion] || 'neutral';

            try {
                vrm.expressionManager.setValue('happy', 0);
                vrm.expressionManager.setValue('sad', 0);
                vrm.expressionManager.setValue('angry', 0);
                vrm.expressionManager.setValue('surprised', 0);
                vrm.expressionManager.setValue('neutral', 0);
                vrm.expressionManager.setValue(expressionName, intensity);

                document.getElementById('emotion-text').textContent =
                    emotion.charAt(0).toUpperCase() + emotion.slice(1);
            } catch (error) {
                console.error('[ERROR] Expression:', error);
            }
        }

        // 嘴型控制
        function setViseme(viseme, weight = 1.0) {
            if (!vrm || !vrm.expressionManager) return;

            const visemeMap = { 'A': 'aa', 'E': 'ee', 'I': 'ih', 'O': 'oh', 'U': 'ou' };
            const vrmViseme = visemeMap[viseme] || 'aa';

            try {
                Object.values(visemeMap).forEach(v => {
                    vrm.expressionManager.setValue(v, 0);
                });
                vrm.expressionManager.setValue(vrmViseme, weight);
            } catch (error) {}
        }

        // WebSocket
        function connectWebSocket() {
            if (ws && (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CONNECTING)) return;

            ws = new WebSocket('ws://localhost:8000/ws');

            ws.onopen = () => {
                console.log('[OK] WebSocket connected');
                updateStatus('connected', 'Ready');
            };

            ws.onmessage = (event) => {
                const data = JSON.parse(event.data);

                if (data.type === 'emotion') {
                    setExpression(data.emotion, data.intensity || 1.0);
                }
                if (data.type === 'audio') {
                    playAudio(data.audio);
                }
            };

            ws.onerror = (error) => {
                console.error('[ERROR] WebSocket:', error);
                updateStatus('disconnected', 'Error');
            };

            ws.onclose = () => {
                updateStatus('disconnected', 'Disconnected');
                ws = null;
                setTimeout(connectWebSocket, 3000);
            };
        }

        function updateStatus(status, text) {
            const statusDot = document.querySelector('.status-dot');
            const statusText = document.getElementById('status-text');

            statusDot.className = 'status-dot';
            if (status === 'connected') statusDot.classList.add('status-connected');
            else if (status === 'thinking') statusDot.classList.add('status-thinking');
            else statusDot.classList.add('status-disconnected');

            statusText.textContent = text;
        }

        // 音频播放
        async function playAudio(audioBase64) {
            if (currentAudio) currentAudio.pause();

            try {
                const audioData = atob(audioBase64);
                const arrayBuffer = new ArrayBuffer(audioData.length);
                const view = new Uint8Array(arrayBuffer);
                for (let i = 0; i < audioData.length; i++) {
                    view[i] = audioData.charCodeAt(i);
                }

                const blob = new Blob([arrayBuffer], { type: 'audio/wav' });
                const audioUrl = URL.createObjectURL(blob);
                currentAudio = new Audio(audioUrl);

                currentAudio.addEventListener('play', () => { animateMouth(); });
                currentAudio.addEventListener('ended', () => {
                    stopMouth();
                    updateStatus('connected', 'Ready');
                });

                await currentAudio.play();
            } catch (error) {
                console.error('[ERROR] Audio:', error);
            }
        }

        let mouthAnimationFrame = null;
        function animateMouth() {
            const visemes = ['A', 'I', 'U', 'E', 'O'];
            let index = 0;
            function animate() {
                setViseme(visemes[index], 0.7);
                index = (index + 1) % visemes.length;
                mouthAnimationFrame = setTimeout(animate, 100);
            }
            animate();
        }

        function stopMouth() {
            if (mouthAnimationFrame) {
                clearTimeout(mouthAnimationFrame);
                mouthAnimationFrame = null;
            }
            setViseme('A', 0);
        }

        // 语音识别
        function initSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!SpeechRecognition) {
                document.getElementById('mic-status').textContent = 'Not supported';
                return;
            }

            recognition = new SpeechRecognition();
            recognition.continuous = false;
            recognition.interimResults = false;
            recognition.lang = 'zh-CN';

            recognition.onstart = () => {
                isListening = true;
                document.getElementById('talk-btn').classList.add('recording');
                document.getElementById('mic-status').textContent = 'Listening...';
                updateStatus('thinking', 'Listening...');
            };

            recognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                sendTextToAI(transcript);
            };

            recognition.onerror = (event) => {
                console.error('[ERROR] Speech:', event.error);
                updateStatus('connected', 'Ready');
                isListening = false;
                document.getElementById('talk-btn').classList.remove('recording');
            };

            recognition.onend = () => {
                isListening = false;
                document.getElementById('talk-btn').classList.remove('recording');
                document.getElementById('mic-status').textContent = 'Click and speak';
            };
        }

        function sendTextToAI(text) {
            if (!ws || ws.readyState !== WebSocket.OPEN) {
                console.error('[ERROR] WebSocket not connected');
                updateStatus('disconnected', 'Not connected');
                return;
            }

            updateStatus('thinking', 'Thinking...');
            ws.send(JSON.stringify({ type: 'user_input', text: text }));
        }

        // 按钮事件
        document.getElementById('talk-btn').addEventListener('mousedown', () => {
            if (recognition && !isListening) {
                try { recognition.start(); } catch (error) { console.error(error); }
            }
        });

        document.getElementById('talk-btn').addEventListener('mouseup', () => {
            if (recognition && isListening) recognition.stop();
        });

        // 动画循环
        function animate() {
            requestAnimationFrame(animate);

            const delta = clock.getDelta();
            time += delta;

            if (vrm) {
                vrm.update(delta);

                // 呼吸和自然动作
                if (vrm.humanoid) {
                    const chest = vrm.humanoid.getNormalizedBoneNode('chest');
                    if (chest) {
                        chest.rotation.x = Math.sin(time * 1.5) * 0.02;
                    }

                    const head = vrm.humanoid.getNormalizedBoneNode('head');
                    if (head) {
                        head.rotation.y = Math.sin(time * 0.35) * 0.05;
                        head.rotation.z = Math.cos(time * 0.28) * 0.03;
                    }

                    // 手臂微动 - 关键：使用 rotation.set() 保持基础姿势
                    const leftArm = vrm.humanoid.getNormalizedBoneNode('leftUpperArm');
                    const rightArm = vrm.humanoid.getNormalizedBoneNode('rightUpperArm');

                    if (leftArm) {
                        leftArm.rotation.set(
                            Math.sin(time * 0.8) * 0.05,
                            0,
                            1.5 + Math.sin(time * 0.6) * 0.03
                        );
                    }

                    if (rightArm) {
                        rightArm.rotation.set(
                            Math.sin(time * 0.8 + Math.PI) * 0.05,
                            0,
                            -1.5 - Math.sin(time * 0.6) * 0.03
                        );
                    }
                }
            }

            controls.update();
            renderer.render(scene, camera);
        }

        // 启动
        window.addEventListener('load', () => {
            initSpeechRecognition();
            connectWebSocket();
            animate();
        });

        // 调试接口
        window.ani = {
            setExpression,
            sendText: sendTextToAI,
            vrm: () => vrm
        };
    </script>
</body>
</html>
