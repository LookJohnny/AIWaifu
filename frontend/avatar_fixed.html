<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ani - AI Voice Companion (FIXED VERSION)</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Arial, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            overflow: hidden;
            color: white;
        }
        #canvas-container { width: 100vw; height: 100vh; position: relative; }
        #controls {
            position: absolute;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0, 0, 0, 0.7);
            padding: 20px 40px;
            border-radius: 50px;
            display: flex;
            gap: 15px;
            align-items: center;
            backdrop-filter: blur(10px);
        }
        button {
            padding: 12px 24px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border: none;
            border-radius: 25px;
            color: white;
            font-size: 14px;
            cursor: pointer;
            transition: all 0.3s;
            font-weight: 600;
        }
        button:hover { transform: translateY(-2px); box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4); }
        button.recording { background: linear-gradient(135deg, #f44336, #e91e63); animation: pulse 1.5s infinite; }
        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }
        #status {
            position: absolute;
            top: 20px;
            left: 20px;
            background: rgba(0, 0, 0, 0.7);
            padding: 15px 25px;
            border-radius: 15px;
            font-size: 14px;
            backdrop-filter: blur(10px);
        }
        .status-dot {
            display: inline-block;
            width: 10px;
            height: 10px;
            border-radius: 50%;
            margin-right: 8px;
            animation: pulse-dot 2s infinite;
        }
        @keyframes pulse-dot {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        .status-connected { background: #4CAF50; }
        .status-disconnected { background: #f44336; }
        .status-thinking { background: #FFC107; }
        #emotion-display {
            position: absolute;
            top: 20px;
            right: 20px;
            background: rgba(0, 0, 0, 0.7);
            padding: 15px 25px;
            border-radius: 15px;
            backdrop-filter: blur(10px);
        }
        #loading {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            text-align: center;
            font-size: 24px;
            font-weight: 600;
        }
        .spinner {
            border: 4px solid rgba(255, 255, 255, 0.3);
            border-top: 4px solid white;
            border-radius: 50%;
            width: 50px;
            height: 50px;
            animation: spin 1s linear infinite;
            margin: 20px auto;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        #debug-info {
            position: absolute;
            bottom: 100px;
            left: 20px;
            background: rgba(0, 0, 0, 0.8);
            padding: 10px 15px;
            border-radius: 10px;
            font-size: 11px;
            font-family: 'Courier New', monospace;
            max-width: 300px;
        }
    </style>
</head>
<body>
    <div id="canvas-container"></div>
    <div id="loading"><div class="spinner"></div>Loading Ani...</div>
    <div id="status" style="display: none;">
        <span class="status-dot status-disconnected"></span>
        <span id="status-text">Disconnected</span>
    </div>
    <div id="emotion-display" style="display: none;">
        <div style="opacity: 0.7; font-size: 12px; margin-bottom: 5px;">Emotion</div>
        <div id="emotion-text">Neutral</div>
    </div>
    <div id="controls" style="display: none;">
        <button id="talk-btn">Hold to Talk</button>
        <span id="mic-status" style="font-size: 12px; opacity: 0.7;">Click and speak</span>
    </div>
    <div id="debug-info" style="display: none;">
        Animation: <span id="anim-status">Idle</span><br>
        Frame: <span id="frame-count">0</span><br>
        Bones: <span id="bone-status">Loading...</span>
    </div>

    <script type="importmap">
    {
        "imports": {
            "three": "https://cdn.jsdelivr.net/npm/three@0.167.0/build/three.module.js",
            "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.167.0/examples/jsm/",
            "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3.1.2/lib/three-vrm.module.js"
        }
    }
    </script>

    <script type="module">
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { VRMLoaderPlugin, VRMUtils } from '@pixiv/three-vrm';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';

        // Global state
        let scene, camera, renderer, controls;
        let currentVRM = null;
        let ws = null;
        let recognition = null;
        let isListening = false;
        let currentAudio = null;
        let frameCount = 0;

        // Emotion to blend shape mapping
        const EMOTION_MAP = {
            'joy': 'happy',
            'sad': 'sad',
            'anger': 'angry',
            'surprise': 'surprised',
            'neutral': 'neutral'
        };

        // Initialize Three.js scene
        function initScene() {
            scene = new THREE.Scene();
            scene.background = new THREE.Color(0x212121);

            camera = new THREE.PerspectiveCamera(30, window.innerWidth / window.innerHeight, 0.1, 100);
            camera.position.set(0, 1.4, 3);

            renderer = new THREE.WebGLRenderer({ antialias: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio);
            renderer.outputColorSpace = THREE.SRGBColorSpace;
            document.getElementById('canvas-container').appendChild(renderer.domElement);

            const directionalLight = new THREE.DirectionalLight(0xffffff, 1.5);
            directionalLight.position.set(1, 1, 1);
            scene.add(directionalLight);

            const ambientLight = new THREE.AmbientLight(0xffffff, 0.8);
            scene.add(ambientLight);

            const rimLight = new THREE.DirectionalLight(0x667eea, 0.5);
            rimLight.position.set(-1, 1, -1);
            scene.add(rimLight);

            controls = new OrbitControls(camera, renderer.domElement);
            controls.target.set(0, 1.2, 0);
            controls.enableDamping = true;
            controls.dampingFactor = 0.05;
            controls.minDistance = 1;
            controls.maxDistance = 10;
            controls.maxPolarAngle = Math.PI / 2;
            controls.update();

            window.addEventListener('resize', onWindowResize);
            console.log('[OK] Scene initialized');
        }

        function onWindowResize() {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        }

        // Load VRM model
        async function loadVRM() {
            const loader = new GLTFLoader();
            loader.register((parser) => new VRMLoaderPlugin(parser));

            try {
                const gltf = await loader.loadAsync('/character/darkhair.vrm');
                const vrm = gltf.userData.vrm;

                if (currentVRM) {
                    scene.remove(currentVRM.scene);
                    VRMUtils.deepDispose(currentVRM.scene);
                }

                currentVRM = vrm;
                scene.add(vrm.scene);
                vrm.scene.rotation.y = Math.PI;

                console.log('[OK] VRM loaded successfully');
                console.log('Humanoid:', vrm.humanoid ? 'YES' : 'NO');

                // Initialize base pose BEFORE any animations
                initializeBasePose();

                // Show available expressions
                if (vrm.expressionManager?.expressionMap) {
                    console.log('Available expressions:', Object.keys(vrm.expressionManager.expressionMap));
                }

                document.getElementById('loading').style.display = 'none';
                document.getElementById('controls').style.display = 'flex';
                document.getElementById('status').style.display = 'block';
                document.getElementById('emotion-display').style.display = 'block';
                document.getElementById('debug-info').style.display = 'block';
                document.getElementById('bone-status').textContent = 'Ready';

            } catch (error) {
                console.error('[ERROR] Failed to load VRM:', error);
                document.getElementById('loading').innerHTML =
                    '<div style="color: #f44336;">Failed to load character<br>Please check console</div>';
            }
        }

        // Base pose rotations (stored for idle animation)
        let basePose = {};

        function initializeBasePose() {
            if (!currentVRM || !currentVRM.humanoid) {
                console.error('[ERROR] Cannot initialize pose - no humanoid');
                return;
            }

            console.log('[INFO] Initializing natural standing pose...');

            // Left arm - natural hanging position
            const leftUpperArm = currentVRM.humanoid.getNormalizedBoneNode('leftUpperArm');
            const leftLowerArm = currentVRM.humanoid.getNormalizedBoneNode('leftLowerArm');

            if (leftUpperArm) {
                leftUpperArm.rotation.set(0.3, 0, 0.8);  // Forward, straight, down
                basePose.leftUpperArm = new THREE.Euler(0.3, 0, 0.8);
                console.log('[OK] Left arm posed');
            }
            if (leftLowerArm) {
                leftLowerArm.rotation.set(0, 0, 0.3);  // Slight bend
            }

            // Right arm - natural hanging position
            const rightUpperArm = currentVRM.humanoid.getNormalizedBoneNode('rightUpperArm');
            const rightLowerArm = currentVRM.humanoid.getNormalizedBoneNode('rightLowerArm');

            if (rightUpperArm) {
                rightUpperArm.rotation.set(0.3, 0, -0.8);  // Forward, straight, down
                basePose.rightUpperArm = new THREE.Euler(0.3, 0, -0.8);
                console.log('[OK] Right arm posed');
            }
            if (rightLowerArm) {
                rightLowerArm.rotation.set(0, 0, -0.3);  // Slight bend
            }

            console.log('[OK] Base pose initialized');
            console.log('Base pose:', basePose);
        }

        // Idle animation time
        let idleTime = 0;

        function updateIdleAnimation(deltaTime) {
            if (!currentVRM || !currentVRM.humanoid) return;

            idleTime += deltaTime;

            // 1. Breathing - chest expansion
            const chest = currentVRM.humanoid.getNormalizedBoneNode('chest');
            if (chest) {
                const breathCycle = Math.sin(idleTime * 1.2) * 0.015;
                chest.rotation.x = breathCycle;
            }

            // 2. Weight shifting
            const hips = currentVRM.humanoid.getNormalizedBoneNode('hips');
            if (hips) {
                const sway = Math.sin(idleTime * 0.4) * 0.02;
                hips.rotation.y = sway;
            }

            // 3. Head movement
            const head = currentVRM.humanoid.getNormalizedBoneNode('head');
            if (head) {
                const headSway = Math.sin(idleTime * 0.35) * 0.05;
                const headTilt = Math.cos(idleTime * 0.28) * 0.03;
                head.rotation.y = headSway;
                head.rotation.z = headTilt;
            }

            // 4. Arm micro-movements - MAINTAIN BASE POSE
            const leftUpperArm = currentVRM.humanoid.getNormalizedBoneNode('leftUpperArm');
            const rightUpperArm = currentVRM.humanoid.getNormalizedBoneNode('rightUpperArm');

            if (leftUpperArm && basePose.leftUpperArm) {
                const armSway = Math.sin(idleTime * 0.6) * 0.03;
                leftUpperArm.rotation.set(
                    basePose.leftUpperArm.x + armSway,
                    basePose.leftUpperArm.y,
                    basePose.leftUpperArm.z + armSway * 0.5
                );
            }

            if (rightUpperArm && basePose.rightUpperArm) {
                const armSway = Math.sin(idleTime * 0.6 + Math.PI) * 0.03;
                rightUpperArm.rotation.set(
                    basePose.rightUpperArm.x + armSway,
                    basePose.rightUpperArm.y,
                    basePose.rightUpperArm.z - armSway * 0.5
                );
            }
        }

        // Set expression
        function setExpression(emotion, intensity = 1.0) {
            if (!currentVRM || !currentVRM.expressionManager) {
                console.warn('[WARN] No VRM or expression manager available');
                return;
            }

            const expressionName = EMOTION_MAP[emotion] || 'neutral';

            try {
                // Reset all expressions
                currentVRM.expressionManager.setValue('happy', 0);
                currentVRM.expressionManager.setValue('sad', 0);
                currentVRM.expressionManager.setValue('angry', 0);
                currentVRM.expressionManager.setValue('surprised', 0);
                currentVRM.expressionManager.setValue('neutral', 0);

                // Set target expression
                currentVRM.expressionManager.setValue(expressionName, intensity);

                console.log(`[Expression] ${emotion} -> ${expressionName} (${intensity.toFixed(2)})`);

                document.getElementById('emotion-text').textContent =
                    emotion.charAt(0).toUpperCase() + emotion.slice(1) +
                    ` (${Math.round(intensity * 100)}%)`;

            } catch (error) {
                console.error('[ERROR] Failed to set expression:', error);
            }
        }

        // Lip-sync viseme system
        function setViseme(viseme, weight = 1.0) {
            if (!currentVRM || !currentVRM.expressionManager) return;

            const visemeMap = {
                'A': 'aa',
                'E': 'ee',
                'I': 'ih',
                'O': 'oh',
                'U': 'ou'
            };

            const vrmViseme = visemeMap[viseme] || 'aa';

            try {
                // Reset all visemes
                Object.values(visemeMap).forEach(v => {
                    currentVRM.expressionManager.setValue(v, 0);
                });
                currentVRM.expressionManager.setValue(vrmViseme, weight);
            } catch (error) {
                // Silently fail if viseme not available
            }
        }

        // WebSocket connection
        function connectWebSocket() {
            if (ws && (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CONNECTING)) {
                console.log('[INFO] WebSocket already connected/connecting');
                return;
            }

            console.log('[INFO] Connecting to WebSocket...');
            ws = new WebSocket('ws://localhost:8000/ws');

            ws.onopen = () => {
                console.log('[OK] WebSocket connected');
                updateStatus('connected', 'Ready');
            };

            ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                console.log('[WS] Received:', data.type);

                if (data.type === 'emotion') {
                    setExpression(data.emotion, data.intensity || 1.0);
                }
                if (data.type === 'audio') {
                    playAudio(data.audio);
                }
            };

            ws.onerror = (error) => {
                console.error('[ERROR] WebSocket error:', error);
                updateStatus('disconnected', 'Error');
            };

            ws.onclose = () => {
                console.log('[INFO] WebSocket closed, reconnecting in 3s...');
                updateStatus('disconnected', 'Disconnected');
                ws = null;
                setTimeout(connectWebSocket, 3000);
            };
        }

        function updateStatus(status, text) {
            const statusDot = document.querySelector('.status-dot');
            const statusText = document.getElementById('status-text');

            statusDot.className = 'status-dot';
            if (status === 'connected') statusDot.classList.add('status-connected');
            else if (status === 'thinking') statusDot.classList.add('status-thinking');
            else statusDot.classList.add('status-disconnected');

            statusText.textContent = text;
        }

        // Audio playback with lip-sync
        async function playAudio(audioBase64) {
            if (currentAudio) {
                currentAudio.pause();
                currentAudio = null;
            }

            try {
                const audioData = atob(audioBase64);
                const arrayBuffer = new ArrayBuffer(audioData.length);
                const view = new Uint8Array(arrayBuffer);
                for (let i = 0; i < audioData.length; i++) {
                    view[i] = audioData.charCodeAt(i);
                }

                const blob = new Blob([arrayBuffer], { type: 'audio/wav' });
                const audioUrl = URL.createObjectURL(blob);
                currentAudio = new Audio(audioUrl);

                currentAudio.addEventListener('play', () => {
                    console.log('[Audio] Playing');
                    animateMouth();
                });

                currentAudio.addEventListener('ended', () => {
                    console.log('[Audio] Ended');
                    stopMouth();
                    updateStatus('connected', 'Ready');
                });

                await currentAudio.play();
            } catch (error) {
                console.error('[ERROR] Audio playback failed:', error);
            }
        }

        let mouthAnimationFrame = null;
        function animateMouth() {
            const visemes = ['A', 'I', 'U', 'E', 'O'];
            let index = 0;

            function animate() {
                setViseme(visemes[index], 0.7);
                index = (index + 1) % visemes.length;
                mouthAnimationFrame = setTimeout(animate, 100);
            }

            animate();
        }

        function stopMouth() {
            if (mouthAnimationFrame) {
                clearTimeout(mouthAnimationFrame);
                mouthAnimationFrame = null;
            }
            setViseme('A', 0);
        }

        // Speech recognition
        function initSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

            if (!SpeechRecognition) {
                console.warn('[WARN] Speech recognition not supported');
                document.getElementById('mic-status').textContent = 'Not supported';
                return;
            }

            recognition = new SpeechRecognition();
            recognition.continuous = false;
            recognition.interimResults = false;
            recognition.lang = 'zh-CN';  // Chinese

            recognition.onstart = () => {
                console.log('[Speech] Recognition started');
                isListening = true;
                document.getElementById('talk-btn').classList.add('recording');
                document.getElementById('mic-status').textContent = 'Listening...';
                updateStatus('thinking', 'Listening...');
            };

            recognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                console.log('[Speech] Recognized:', transcript);
                sendTextToAI(transcript);
            };

            recognition.onerror = (event) => {
                console.error('[ERROR] Speech recognition error:', event.error);
                updateStatus('connected', 'Ready');
                isListening = false;
                document.getElementById('talk-btn').classList.remove('recording');
            };

            recognition.onend = () => {
                console.log('[Speech] Recognition ended');
                isListening = false;
                document.getElementById('talk-btn').classList.remove('recording');
                document.getElementById('mic-status').textContent = 'Click and speak';
            };

            console.log('[OK] Speech recognition initialized');
        }

        function sendTextToAI(text) {
            if (!ws || ws.readyState !== WebSocket.OPEN) {
                console.error('[ERROR] WebSocket not connected');
                updateStatus('disconnected', 'Not connected');
                return;
            }

            console.log('[Send] User input:', text);
            updateStatus('thinking', 'Thinking...');
            ws.send(JSON.stringify({ type: 'user_input', text: text }));
        }

        // Button event listeners
        document.getElementById('talk-btn').addEventListener('mousedown', () => {
            if (recognition && !isListening) {
                try {
                    recognition.start();
                } catch (error) {
                    console.error('[ERROR] Failed to start recognition:', error);
                }
            }
        });

        document.getElementById('talk-btn').addEventListener('mouseup', () => {
            if (recognition && isListening) {
                recognition.stop();
            }
        });

        // Animation loop
        const clock = new THREE.Clock();

        function animate() {
            requestAnimationFrame(animate);

            const deltaTime = clock.getDelta();
            frameCount++;

            if (currentVRM) {
                // Update VRM (required for expressions, etc.)
                currentVRM.update(deltaTime);

                // Update idle animation
                updateIdleAnimation(deltaTime);
            }

            // Update camera controls
            controls.update();

            // Render scene
            renderer.render(scene, camera);

            // Update debug info
            if (frameCount % 60 === 0) {
                document.getElementById('frame-count').textContent = frameCount;
                document.getElementById('anim-status').textContent =
                    currentAudio ? 'Talking' : 'Idle';
            }
        }

        // Initialize on load
        window.addEventListener('load', async () => {
            console.log('[INFO] Initializing Ani...');
            initScene();
            await loadVRM();
            initSpeechRecognition();
            connectWebSocket();
            animate();
            console.log('[OK] Ani initialized successfully');
        });

        // Expose functions for debugging
        window.ani = {
            setExpression,
            sendText: sendTextToAI,
            vrm: () => currentVRM,
            resetPose: initializeBasePose
        };

        console.log('[INFO] Debug functions available: window.ani.setExpression(), window.ani.sendText(), window.ani.vrm(), window.ani.resetPose()');
    </script>
</body>
</html>
